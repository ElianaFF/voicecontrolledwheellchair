{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning\n",
    "#### Voice-Controlled Wheelchair Project\n",
    "##### Eliana Ferreira\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2\n",
    "import os\n",
    "\n",
    "dataset_path = 'dataset'\n",
    "\n",
    "classes = ['forward', 'backward', 'left', 'right', 'stop', '_silence_', '_unknown_']\n",
    "\n",
    "# iniciar um dicionário com as diferentes classes \n",
    "classe_ficheiros = {}\n",
    "\n",
    "for class_name in classes: \n",
    "    class_folder = os.path.join(dataset_path, class_name) # ir buscar o path completo de cada classe\n",
    "    ficheiros = os.listdir(class_folder) # vai buscar todos os ficheiros daquela pasta\n",
    "    classe_ficheiros[class_name] = ficheiros # atribuir os ficheiros à respetiva classe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à pergunta 2.3 \n",
    "\n",
    "O desequilíbrio nas classes pode gerar uma série de desafios. Classes maioritárias, como \"Left\", \"Right\" e \"Stop\", tendem a dominar as análises estatísticas, enquanto classes minoritárias, como \"Silence\" e \"Unknown\", podem não conter exemplos suficientes para uma extração de características adequada ou para a deteção de outliers. Durante a normalização e transformação de dados, técnicas como PCA e métodos de seleção de características, como Fisher Score ou ReliefF, podem priorizar características associadas às classes dominantes, uma vez que estas contribuem mais para a variância global, ignorando informações cruciais para classes menos representadas. Esse desequilíbrio também afeta a divisão do conjunto de dados em treino, validação e teste, onde divisões aleatórias podem resultar em representações insuficientes das classes minoritárias, levando a generalizações fracas e métricas de desempenho distorcidas, como uma precisão aumentada que não reflete a capacidade real do modelo reconhecer todas as classes. No caso de classificadores como o KNN, o viés para as classes dominantes torna-se evidente, pois os vizinhos mais próximos de um ponto de consulta geralmente pertencem às classes maioritárias. Isso é exacerbado por valores grandes de k, que amplificam ainda mais o peso dessas classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "\n",
    "def load_and_normalize(ficheiro, class_folder):\n",
    "    ficheiro_path = os.path.join(class_folder, ficheiro) # cria um path para o ficheiro\n",
    "    fs, data = wavfile.read(ficheiro_path) # carrega o ficheiro \n",
    "    data = data / np.iinfo(data.dtype).max  # normaliza o som\n",
    "    return fs, data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5\n",
    "import os \n",
    "\n",
    "def envelope(sinal, window_size):\n",
    "    abs_signal = np.abs(sinal) # meter todos os valores do sinal em absoluto\n",
    "    half_window = window_size // 2 \n",
    "    padded_signal = np.pad(abs_signal, (half_window, half_window), mode='constant', constant_values=0) # padding de zero no inicio e fim para que hajam valores para a moving window, o número de zeros é igual ao número da halfwindow\n",
    "    envelope_signal = np.zeros_like(sinal)  # criar uma array com as mesmas caracteristicas que o sinal, cheia de zeros\n",
    "    for i in range(len(sinal)):\n",
    "        envelope_signal[i] = np.mean(padded_signal[i:i + window_size]) # cria subarrays que começam em i e terminam em i + window para criar a window       \n",
    "    return envelope_signal\n",
    "\n",
    "# exemplo só para visualizar \n",
    "# carregar o primeiro ficheiro de forward\n",
    "fs, data = load_and_normalize('0a2b400e_nohash_0.wav', 'dataset/forward')\n",
    "\n",
    "# atribuir valores a window (ímpar - 101) e a sinal (data)\n",
    "window_size = 35\n",
    "env = envelope(data, window_size)\n",
    "\n",
    "# Visualizar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "time = np.arange(len(data)) / fs #dá-nos o tempo de cada sinal\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time, data, label='Sinal Original')\n",
    "plt.plot(time, env, label='Envelope', color='red', linewidth=2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Sinal e Envelope')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6\n",
    "\n",
    "import random\n",
    "\n",
    "def visualizar_samples(classe_ficheiros, dataset_path, fs):\n",
    "    classes = list(classe_ficheiros.keys())\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    for i, class_name in enumerate(classes): # o enumerar resulta no índice e no elemento\n",
    "        \n",
    "        selected_file = random.choice(classe_ficheiros[class_name]) # selecionar um ficheiro random da classe dada\n",
    "        class_folder = os.path.join(dataset_path, class_name) \n",
    "        _, data = load_and_normalize(selected_file, class_folder) # wavfile.read - devolve um tuple - fs,data - porque queremos só a segunda informação metemos só o placeholder\n",
    "        time = np.arange(len(data)) / fs  # /fs converte os indices em tempo(de cada sample), uma vez ~que fs é a frequencia do sinal (nº de samples por segundo)\n",
    "        duration_limit = min(len(data), fs) # ou 1 segundo (fs) ou então o comprimento do sinal, usamos min para se o sinal for menor que um segundo, ficar essa duração\n",
    "        data = data[:duration_limit]\n",
    "        time = time[:duration_limit] # para fazer plot - time e data - têm o mesmo número de elementos - se não o python levantava um ValueError porque a função plot assume que cada coordenada x tem de ter uma correspondente coordenada y\n",
    "\n",
    "        plt.subplot(7,1, i + 1)  # 4 linhas, 2 colunas, sequencial\n",
    "        plt.plot(time, data)\n",
    "        plt.title(class_name.capitalize())\n",
    "        plt.xlabel('Time [s]')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.subplots_adjust(hspace=1.7)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "visualizar_samples(classe_ficheiros, dataset_path, fs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1\n",
    "\n",
    "def extract_feature1(data, envelope_signal, fs, threshold=0.035):\n",
    "    start_idx = np.argmax(envelope_signal > threshold) # argmax devolve o primeiro valor que se adequa à condição\n",
    "    end_idx = len(envelope_signal) - np.argmax(envelope_signal[::-1] > threshold) - 1 # procura o primeiro indice a passar o threshold na array invertida (ultimo na original\n",
    "    # depois para voltar ao indice na original, subtrai o indice encontrado ao length e subtrai um porque começa em 0\n",
    "    if np.max(envelope_signal) <= threshold:  # se nunca passar o threshold o incio é zero e o fim é o length - 1\n",
    "        start_idx = 0\n",
    "        end_idx = len(data) - 1\n",
    "\n",
    "    duration = (end_idx - start_idx + 1) / fs # divide-se pela frequencia para ter o tempo porque T=1/fs \n",
    "    return duration #F1\n",
    "\n",
    "#exemplo \n",
    "fs, data = load_and_normalize('0b7ee1a0_nohash_2.wav', 'dataset/forward')\n",
    "env= envelope(data, window_size)\n",
    "dur = extract_feature1(data, env, fs, threshold=0.035)\n",
    "print(dur)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 \n",
    "\n",
    "def extract_feature2_e_feature3(envelope_signal, start_idx, end_idx):\n",
    "    envelope_segment = envelope_signal[start_idx:end_idx] # obtém o sinal correspondente à duration (F1)\n",
    "    F2 = np.mean(envelope_segment) # média dos valores no intervalo definido anteriormente\n",
    "    F3 = np.std(envelope_segment) # desvio padrão dos valores no intervalo definido anteriormente \n",
    "\n",
    "    return F2, F3\n",
    "\n",
    " # tem que se definir start_idx e end_idx para dar call à função\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3\n",
    "\n",
    "def extract_feature4(data, start_idx, end_idx):\n",
    "   \n",
    "    signal_segment = data[start_idx:end_idx]\n",
    "\n",
    "    energy = np.sum(np.square(signal_segment)) # soma do quadrado das amplitudes no intervalo definido anteriormente (acima do threshold)\n",
    "\n",
    "    return energy #F4\n",
    "\n",
    "# tem que se definir start_idx e end_idx para dar call à função\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.4\n",
    "\n",
    "def extract_features5to24(data, start_idx, end_idx, num_bins=20):\n",
    "    signal_segment = data[start_idx:end_idx]\n",
    "    bin_size = len(signal_segment) // num_bins # perceber o tamanho de cada bin dividindo o sinal acima do threshold pelo nº de bins\n",
    "    energies = [] \n",
    "    for i in range(num_bins):\n",
    "       \n",
    "        bin_start = i * bin_size\n",
    "        bin_end = (i + 1) * bin_size if i < num_bins - 1 else len(signal_segment) # (i + 1) * bin_size exceto o ultimo bin, para o ultimo é o comprimento final do segmento\n",
    "\n",
    "        # extrair o segmento do bin atual\n",
    "        bin_signal = signal_segment[bin_start:bin_end]\n",
    "\n",
    "        # calcular a energia do bin\n",
    "        bin_energy = np.sum(np.square(bin_signal))\n",
    "\n",
    "        # juntar a energia do bin à array energies \n",
    "        energies.append(bin_energy)\n",
    "\n",
    "    return energies\n",
    "\n",
    "   # tem que se definir start_idx e end_idx para dar call à função\n",
    "   # F5 to F24 são as energias dos 20 bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.5\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_feature_matrices(classe_ficheiros, dataset_path, fs, num_features=24):\n",
    "    total_sounds = sum(len(ficheiros) for ficheiros in classe_ficheiros.values())\n",
    "    X = np.zeros((total_sounds, num_features))  # criar uma matriz com os ficheiros no y e as features no x\n",
    "    Y = np.zeros((total_sounds, 1))  # criar uma matriz com os ficheiros no y e uma coluna para depois colocar a classe \n",
    "\n",
    "    sound_index = 0  # para atualizar a linha atual \n",
    "\n",
    "    for class_label, class_name in enumerate(classe_ficheiros.keys()):  # Enumerar para atribuir um número a cada classe\n",
    "        class_folder = os.path.join(dataset_path, class_name)\n",
    "        \n",
    "        for ficheiro in classe_ficheiros[class_name]:\n",
    "        \n",
    "            fs, data = load_and_normalize(ficheiro, class_folder)\n",
    "\n",
    "            # Calcular o envelope\n",
    "            window_size = 101\n",
    "            envelope_signal = envelope(data, window_size)\n",
    "\n",
    "            # Extrair features\n",
    "            F1 = extract_feature1(data, envelope_signal, fs)  # Duration\n",
    "            F2 = np.mean(envelope_signal)  # Mean of envelope\n",
    "            F3 = np.std(envelope_signal)  # Standard deviation of envelope \n",
    "            start_idx = np.argmax(envelope_signal > 0.1)\n",
    "            end_idx = len(envelope_signal) - np.argmax(envelope_signal[::-1] > 0.1) - 1\n",
    "            F4 = extract_feature4(data, start_idx, end_idx)  # Energia \n",
    "            F5_to_F24 = extract_features5to24(data, start_idx, end_idx)  # Energy bins\n",
    "            \n",
    "            # Combinar todas as features \n",
    "            features = [F1, F2, F3, F4] + F5_to_F24\n",
    "        \n",
    "\n",
    "            # X é das features e Y é das class labels \n",
    "            X[sound_index, :] = features\n",
    "            Y[sound_index, 0] = class_label  # Labels\n",
    "            sound_index += 1\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Example usage\n",
    "X, Y = create_feature_matrices(classe_ficheiros, dataset_path, fs)\n",
    "\n",
    "\n",
    "print(\"Feature Matrix X:\")\n",
    "print(X)\n",
    "print(\"Class Labels Y:\")\n",
    "print(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar as matrizes com pickle \n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "filename = \"matrices.pkl\"\n",
    "filepath = os.path.join(current_dir, filename)\n",
    "with open(filepath, 'wb') as file:\n",
    "    pickle.dump((X, Y), file)\n",
    "\n",
    "with open(filepath, 'rb') as file:\n",
    "    X_loaded, Y_loaded = pickle.load(file)\n",
    "\n",
    "print(\"X_loaded shape:\", X_loaded.shape)\n",
    "print(\"Y_loaded shape:\", Y_loaded.shape)\n",
    "\n",
    "print(Y_loaded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6\n",
    "from scipy.stats import shapiro, kruskal, f_oneway\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def verificar_normalidade(X, Y):\n",
    "    n_features = X.shape[1] #nº de features\n",
    "    results = []\n",
    "    classes = np.unique(Y) # agrupar as classes\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        feature_name = f\"Feature {feature + 1}\"\n",
    "        feature_results = {\n",
    "            \"Feature Name\": feature_name,\n",
    "            \"Normality p-value\": None,\n",
    "            \"Test Applied\": None,\n",
    "            \"Test p-value\": None\n",
    "        }\n",
    "\n",
    "        for classe in classes:\n",
    "            # seleciona os sons da feature atual de X que têm a classe atual em Y atribuída # flatten - torna array \n",
    "            feature_values = X[Y.flatten() == classe, feature]\n",
    "\n",
    "            _, p_value_normalidade = shapiro(feature_values) # porque shapiro devolve dois valores\n",
    "            feature_results[\"Normality p-value\"] = p_value_normalidade\n",
    "\n",
    "            if p_value_normalidade < 0.05:\n",
    "                feature_results[\"Test Applied\"] = \"Kruskal-Wallis\"\n",
    "                stat, test_p_value = kruskal(*feature_values)\n",
    "                feature_results[\"Test p-value\"] = test_p_value\n",
    "            else:\n",
    "                feature_results[\"Test Applied\"] = \"ANOVA\"\n",
    "                _, anova_p_value = f_oneway(*feature_values)\n",
    "                feature_results[\"Test p-value\"] = anova_p_value\n",
    "\n",
    "            results.append(feature_results)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df.sort_values(by=\"Test p-value\", ascending=True)\n",
    "\n",
    "    print(\"\\nRanked Features by Statistical Significance:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "verificar_normalidade(X, Y)\n",
    "\n",
    "#para vermos que a distribuição não é normal\n",
    "def plot_example(X, Y, feature_index=6, class_value=5):\n",
    "\n",
    "    feature_values = X[Y.flatten() == class_value, feature_index]\n",
    "\n",
    "    # criar histograma\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(feature_values, bins=20, edgecolor='black')\n",
    "    plt.title(f'Distribuição da Classe {class_value} na Feature {feature_index + 1} ')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()\n",
    "\n",
    "plot_example(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6.1 \n",
    "\n",
    "O teste de Kruskal-Wallis H é o teste estatístico mais apropriado para esta situação, uma vez que o objetivo principal é avaliar se uma determinada feature pode discriminar significativamente entre diferentes classes, dado que os dados não seguem uma distribuição normal. Ao contrário dos testes paramétricos, como a ANOVA, que exigem a suposição de normalidade, o teste de Kruskal-Wallis é um método não paramétrico que compara as distribuições da feature entre várias classes, sem qualquer suposição de normalidade. Se houvesse apenas duas classes, poderia ser utilizado o teste de Mann-Whitney U, mas existem 7 classes.\n",
    "Ao classificar os dados e comparar as classificações entre as classes, o teste de Kruskal-Wallis avalia efetivamente se as distribuições da feature diferem significativamente entre os grupos, proporcionando assim um método robusto para identificar as features que podem distinguir entre as classes. A capacidade do teste de lidar com dados não normais enquanto compara múltiplos grupos torna-o uma escolha ideal para garantir que o poder discriminatório da feature seja adequadamente avaliado em dados do mundo real, onde a normalidade nem sempre pode ser garantida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 \n",
    "#univariada por classe\n",
    "def detect_outliers(X, Y, feature_index, method='iqr', iqr_multiplier=1.5, z_threshold=3.0):\n",
    "  \n",
    "    classes = np.unique(Y)  \n",
    "    feature_values = X[:, feature_index]  \n",
    "    outliers_per_class = {}  \n",
    "\n",
    "    plt.figure(figsize=(10, 6))  \n",
    "\n",
    "    for i, classe in enumerate(classes): # filtrar os valores de cada classe\n",
    "        class_indexes = np.where(Y.flatten() == classe)[0]\n",
    "        class_values = feature_values[class_indexes]\n",
    "\n",
    "        if method == 'iqr':\n",
    "            Q1 = np.percentile(class_values, 25)  \n",
    "            Q3 = np.percentile(class_values, 75)  \n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - (iqr_multiplier * IQR)\n",
    "            upper_bound = Q3 + (iqr_multiplier * IQR)\n",
    "            outliers_indexes = np.where((class_values < lower_bound) | (class_values > upper_bound))[0]\n",
    "\n",
    "        elif method == 'zscore':\n",
    "            mean = np.mean(class_values)\n",
    "            std = np.std(class_values)\n",
    "            z_scores = (class_values - mean) / std\n",
    "            outliers_indexes = np.where(np.abs(z_scores) > z_threshold)[0]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Use 'iqr' or 'zscore'.\")\n",
    "\n",
    "        # Cir buscar os indices especificos das classes e passá-los para indices globais\n",
    "        global_outliers_indexes = class_indexes[outliers_indexes]\n",
    "\n",
    "        # densidade\n",
    "        no = len(global_outliers_indexes)  # nº de outliers\n",
    "        nr = len(class_values)  # nº total de valores numa classe\n",
    "        outliers_density = (no / nr) * 100\n",
    "\n",
    "        outliers_per_class[classe] = {\n",
    "            \"outliers_density\": outliers_density,\n",
    "            \"outliers_indexes\": global_outliers_indexes.tolist()\n",
    "        }\n",
    "\n",
    "        print(f\"Class {classe}:\")\n",
    "        print(f\"  Outliers Density: {outliers_density:.2f}%\")\n",
    "        print(len(outliers_indexes))\n",
    "\n",
    "    \n",
    "        class_inliers = np.setdiff1d(class_indexes, global_outliers_indexes)\n",
    "        plt.scatter(\n",
    "            np.full_like(class_inliers, i, dtype=int),\n",
    "            feature_values[class_inliers],\n",
    "            color=\"blue\",\n",
    "            alpha=0.6,\n",
    "            label=f\"Class {classe} (Inliers)\" if i == 0 else \"\",\n",
    "        )\n",
    "        plt.scatter(\n",
    "            np.full_like(global_outliers_indexes, i, dtype=int),\n",
    "            feature_values[global_outliers_indexes],\n",
    "            color=\"red\",\n",
    "            alpha=0.6,\n",
    "            label=f\"Class {classe} (Outliers)\" if i == 0 else \"\",\n",
    "            edgecolors=\"black\",\n",
    "        )\n",
    "\n",
    "    # Plot details\n",
    "    plt.title(f\"Feature {feature_index + 1} ({method.upper()} Method)\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Feature Value\")\n",
    "    plt.xticks(range(len(classes)), classes)  # Display class labels on x-axis\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return outliers_per_class\n",
    "\n",
    "detect_outliers(X, Y, 1, method='zscore', z_threshold=3.0)\n",
    "detect_outliers(X, Y, 1, method='iqr', iqr_multiplier=1.5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à pergunta 4.2.3\n",
    "\n",
    "A densidade de outliers calculada pelo método IQR tende a ser superior àquela obtida pelo método z-score devido às diferenças nas abordagens de detecção de outliers. O método IQR identifica outliers com base na amplitude interquartil (a diferença entre o 75º e o 25º percentil) e, com um multiplicador de 1.5, qualquer valor fora do intervalo determinado por essa amplitude é considerado um outlier. Esse critério tende a ser mais sensível a valores extremos, resultando numa maior densidade de outliers, especialmente quando a distribuição dos dados tem caudas mais longas ou dispersão considerável. Por outro lado, o z-score calcula a distância padrão de cada ponto em relação à média. Valores com um z-score superior a 3, por exemplo, são considerados outliers. Quanto maior o valor do z-score (como 3.5 ou 4), maior a probabilidade de um ponto ser classificado como outlier, pois a margem para a variação em torno da média é reduzida, tornando a detecção de outliers mais rigorosa. Isso explica o aumento da densidade de outliers quando o z-threshold é aumentado, já que menos valores ficam dentro do intervalo de confiança definido pelo z-score, resultando em mais pontos sendo considerados outliers. Portanto, enquanto o IQR é mais sensível a dispersões extremas, o z-score tende a ser mais conservador, e ao aumentar o valor do z-threshold, a densidade de outliers diminui, já que a definição de outlier se torna mais restritiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#criar histogramas para cada classe\n",
    "\n",
    "def plot_histograms_per_class(X_loaded, Y_loaded):\n",
    "    classes = np.unique(Y_loaded)  \n",
    "    feature_1 = X_loaded[:, 0]  \n",
    "    feature_2 = X_loaded[:, 1]\n",
    "    print(feature_1)  \n",
    "\n",
    "    # Feature 1\n",
    "    for classe in classes:\n",
    "        values = feature_1[Y_loaded.flatten() == classe]  \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(values, bins=20, color='pink', edgecolor='black')\n",
    "        plt.title(f\"Histograma da Feature 1 para a Classe {classe}\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(axis=\"y\")\n",
    "        plt.show()\n",
    "\n",
    "    # Feature 2 \n",
    "    for classe in classes:\n",
    "        values = feature_2[Y_loaded.flatten() == classe]  # Filter Feature 2 values by class\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(values, bins=20, edgecolor='black')\n",
    "        plt.title(f\"Histograma da Feature 2 para a Classe {classe}\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(axis=\"y\")\n",
    "        plt.show()\n",
    "\n",
    "# Fazer histogramas F1 e F2\n",
    "plot_histograms_per_class(X_loaded, Y_loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3 \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "F1 = X[:, 0]\n",
    "F2 = X[:, 1]\n",
    "\n",
    "def kmeans_clustering(F1, F2, num_clusters):\n",
    "\n",
    "    features = np.column_stack((F1, F2)) # juntas as features numa matriz 2D\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Calcula as distâncias dos pontos ao centróide do cluster ao qual pertencem\n",
    "    distances = np.linalg.norm(features - centroids[labels], axis=1)\n",
    "\n",
    "    # Define um limiar com base no percentil\n",
    "    outliers_indexes = []\n",
    "    for i in range(num_clusters):\n",
    "        cluster_idxs = np.where(labels==i)[0]\n",
    "        \n",
    "        #de indices dos cluster a indices dos outliers no cluster para indices globais\n",
    "        distance_threshold = np.percentile(distances[cluster_idxs], 99.95)\n",
    "        cluster_outliers_indexes = np.where(distances[cluster_idxs] > distance_threshold)[0]\n",
    "        outliers_indexes.extend(cluster_idxs[cluster_outliers_indexes])\n",
    "    \n",
    "    # Identifica os índices dos outliers\n",
    "    outliers = features[outliers_indexes]\n",
    "    print(len(outliers))\n",
    "    \n",
    "    return outliers_indexes, labels, features, centroids, outliers\n",
    "\n",
    "outliers_indexes, labels, features, centroids, outliers = kmeans_clustering(F1, F2, num_clusters=10)\n",
    "\n",
    "# Plot dos resultados \n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the clusters\n",
    "for cluster in np.unique(labels):\n",
    "    cluster_points = features[labels == cluster]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\", alpha=0.6)\n",
    "\n",
    "# Outliers a vermelho\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], color=\"red\", label=\"Outliers\", edgecolor=\"black\", s=50)\n",
    "\n",
    "# Plot dos centroides\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], color=\"black\", marker=\"x\", s=50, label=\"Centroids\")\n",
    "\n",
    "plt.title(\"KMeans Clustering with Outliers\")\n",
    "plt.xlabel(\"Feature 1 (F1)\")\n",
    "plt.ylabel(\"Feature 2 (F2)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "kmeans_clustering(F2, F1, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à perguta 4.4\n",
    "\n",
    "Ao aplicar o algoritmo K-means para identificar os outliers no conjunto de dados, com diferentes números de clusters, foi possível observar que os 14 outliers identificados são pontos que se encontram mais dispersos dentro do espaço de cada cluster, distantes dos seus centróides. Esses outliers representam amostras atípicas, cuja distância em relação aos clusters é significativamente maior, o que foi determinado com base no limiar de 99.95 das distâncias. Esse limiar é bastante restritivo, o que explica a identificação de um número reduzido de outliers. A análise com diferentes números de clusters mostrou que, ao variar a quantidade de agrupamentos, a dispersão dentro dos clusters pode aumentar ou diminuir, afetando a identificação dos outliers. A visualização gráfica dos resultados ilustra bem como os outliers se distribuem nas extremidades dos clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def dbscan_clustering(values, eps, min_samples):\n",
    " \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(values)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def plot_dbscan_results(values, labels):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    unique_labels = set(labels)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            col = 'k'\n",
    "        class_member_mask = (labels == k)\n",
    "        xy = values[class_member_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=6)\n",
    "    \n",
    "    plt.title('DBSCAN Clustering Results')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "features = np.column_stack((F1, F2))\n",
    "\n",
    "eps = 0.5  \n",
    "min_samples = 5  \n",
    "labels = dbscan_clustering(features, eps, min_samples)\n",
    "\n",
    "plot_dbscan_results(features, labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à pergunta 4.6\n",
    "\n",
    "A principal diferença entre as abordagens univariada e multivariada para detecção de outliers está na forma como consideram as variáveis. A abordagem univariada, como o IQR e o Z-Score, analisa cada variável de forma isolada, o que a torna simples e eficaz em identificar outliers em dados unidimensionais ou em casos onde as variáveis não estão inter-relacionadas. No entanto, essa abordagem não captura as interações entre as variáveis, o que pode ser uma limitação significativa em dados multidimensionais, onde as variáveis podem estar correlacionadas. Já a abordagem multivariada, como o DBSCAN e o K-Means, considera simultaneamente todas as variáveis e as suas interações, permitindo identificar outliers em dados complexos e multidimensionais. O DBSCAN, por exemplo, é eficaz em identificar clusters de forma irregular e detectar outliers em regiões de baixa densidade, enquanto o K-Means, embora útil para dados bem agrupados, pode ser sensível a outliers devido ao uso de centróides. Além disso, enquanto a abordagem univariada é mais rápida e simples de aplicar, ela pode não ser capaz de lidar com estruturas de dados mais complexas, onde as interações entre as variáveis são importantes. Por outro lado, as abordagens multivariadas, embora mais computacionalmente exigentes, são mais adequadas para identificar padrões e outliers em dados onde as variáveis influenciam umas às outras. Assim, a escolha entre essas abordagens depende da natureza dos dados: se são simples e independentes, a univariada pode ser suficiente; se são complexos e interdependentes, as abordagens multivariadas são mais eficazes. Neste caso mostra-se mais eficaz a abordagem multivariada para então expressar essa relação entre as features e resultar em outliers reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 \n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def normalize_zscore(data, axis=0):\n",
    "    mean = np.mean(data, axis=axis, keepdims=True)  # média de cada coluna\n",
    "    std = np.std(data, axis=axis, keepdims=True)    # desvio padrao\n",
    "\n",
    "    std[std == 0] = 1 # devolver 1 quanto a divisão +e feita por zero\n",
    "\n",
    "    return (data - mean) / std   #devolve o z-score\n",
    "\n",
    "\n",
    "def pca_with_explained_variance(data, variance_threshold=0.97):\n",
    "\n",
    "    data = normalize_zscore(data)\n",
    " \n",
    "    # centralizar \n",
    "    mean = np.mean(data, axis=0)\n",
    "    centered_data = data - mean\n",
    "\n",
    "    # calcular a matriz de covariância\n",
    "    covariance_matrix = np.cov(centered_data, rowvar=False)\n",
    "\n",
    "    #  eigenvalues e eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # ordem descrescente\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # variância explicada cumulativa\n",
    "    total_variance = np.sum(eigenvalues)\n",
    "    explained_variance_ratio = eigenvalues / total_variance\n",
    "    print(\"Cumulative Variance (per component):\")\n",
    "    for i, var in enumerate(explained_variance_ratio, start=1):\n",
    "        print(f\"Component {i}: {var:.4f}\")\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "   \n",
    "    # nº de componentes para a variancia explicada desejada\n",
    "    n_components = np.searchsorted(cumulative_variance, variance_threshold) + 1\n",
    "\n",
    "    # selecionar os eigenvectors correspondentes\n",
    "    principal_components = eigenvectors[:, :n_components]\n",
    "    print(n_components)\n",
    "\n",
    "    # projetar esses dados nesses vetores\n",
    "    transformed_data = np.dot(centered_data, principal_components)\n",
    "\n",
    "    return transformed_data, principal_components, eigenvalues, n_components\n",
    "\n",
    "pca_with_explained_variance(X, variance_threshold=0.90)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à pergunta 5.1.1\n",
    "\n",
    "Ao realizar a Análise de Componentes Principais (PCA) nos dados, a importância de cada componente pode ser observada através da variância explicada por cada um dos eigenvectors (componentes principais). As componentes com maiores eigenvalues são responsáveis por explicar a maior parte da variabilidade dos dados. Nos resultados apresentados, a primeira componente explica 57.70% da variabilidade, a segunda componente contribui com 13.37%, e a terceira com 8.82%. As componentes seguintes explicam progressivamente menos variabilidade. Para explicar 90% da variabilidade dos dados, são necessárias as seis primeiras componentes, já que a soma da variância cumulativa das seis primeiras componentes atinge os 90.04%. Isso significa que, para preservar a maior parte da informação dos dados e reduzir a dimensionalidade sem perder muita variação, deveríamos utilizar as seis primeiras variáveis transformadas. A normalização dos dados com o z-score antes da aplicação da PCA foi crucial, pois garantiu que todas as variáveis tivessem a mesma escala, permitindo uma comparação justa entre elas ao calcular a variância explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicar o fisherscore\n",
    "\n",
    "#normalizar primeiro!\n",
    "\n",
    "def fisher_score(X, Y):\n",
    "\n",
    "    normalize_zscore(X)\n",
    "\n",
    "    Y = Y.flatten() \n",
    "    unique_classes = np.unique(Y)\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # array para os fisher scores\n",
    "    fisher_scores = np.zeros(n_features)\n",
    "\n",
    "    # média de cada feature\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "\n",
    "    for feature_idx in range(n_features):\n",
    "        inter_class_var = 0\n",
    "        intra_class_var = 0\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            # ficheiros de cada classe\n",
    "            X_cls = X[Y == cls]\n",
    "            \n",
    "            # média e variância dentro de cada classe\n",
    "            cls_mean = np.mean(X_cls[:, feature_idx])\n",
    "            cls_var = np.var(X_cls[:, feature_idx])\n",
    "\n",
    "            # tamanho da classe\n",
    "            cls_size = X_cls.shape[0]\n",
    "\n",
    "            inter_class_var += cls_size * ((cls_mean - overall_mean[feature_idx]) ** 2)\n",
    "            intra_class_var += cls_size * cls_var\n",
    "\n",
    "        # evitar divisão por 0\n",
    "        if intra_class_var != 0:\n",
    "            fisher_scores[feature_idx] = inter_class_var / intra_class_var\n",
    "        else:\n",
    "            fisher_scores[feature_idx] = 0\n",
    "\n",
    "    return fisher_scores\n",
    "\n",
    "fisher_scores = fisher_score(X, Y)\n",
    "top_10_fisher = np.argsort(fisher_scores)[-10:][::-1]\n",
    "print(\"Top 10 features based on Fisher Score:\", top_10_fisher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aplicar o refiefF\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def reliefF(X, Y, k=10):\n",
    "\n",
    "    normalize_zscore(X)\n",
    "\n",
    "    Y = Y.flatten() \n",
    "    n_samples, n_features = X.shape\n",
    "    relief_scores = np.zeros(n_features)\n",
    "    unique_classes = np.unique(Y)\n",
    "\n",
    "    # knn para descobrir vizinhos mais próximos\n",
    "    knn = NearestNeighbors(n_neighbors=k + 1) #porque a amostra em si também conta\n",
    "    knn.fit(X)\n",
    "    neighbors = knn.kneighbors(X, return_distance=False)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        target_class = Y[i]\n",
    "        same_class_neighbors = neighbors[i][Y[neighbors[i]] == target_class][1:k+1]\n",
    "        diff_class_neighbors = neighbors[i][Y[neighbors[i]] != target_class][:k]\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            # recompensar os que são da mesma classe\n",
    "            if len(same_class_neighbors) > 0:\n",
    "                relief_scores[feature_idx] -= np.sum(\n",
    "                    np.abs(X[i, feature_idx] - X[same_class_neighbors, feature_idx])\n",
    "                ) / len(same_class_neighbors)\n",
    "\n",
    "            # penalizar os que são de classe diferente \n",
    "            if len(diff_class_neighbors) > 0:\n",
    "                relief_scores[feature_idx] += np.sum(\n",
    "                    np.abs(X[i, feature_idx] - X[diff_class_neighbors, feature_idx])\n",
    "                ) / len(diff_class_neighbors)\n",
    "\n",
    "    return relief_scores\n",
    "\n",
    "relief_scores = reliefF(X, Y)\n",
    "top_10_relief = np.argsort(relief_scores)[-10:][::-1]\n",
    "print(\"Top 10 features based on Relief Score:\", top_10_relief)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à pergunta 5.2.1\n",
    "\n",
    "Top 10 features based on Fisher Score: [ 0  4  5  1  2  6  7  8 23  3 ]\n",
    "\n",
    "Top 10 features based on Relief Score: [ 9 14 11  4 12 15  3 13 23  8]\n",
    "\n",
    "A diferença nos top 10 features identificados pelo Fisher Score e pelo ReliefF decorre das abordagens distintas que cada método adota para avaliar a importância das características. O Fisher Score é baseado na ideia de separar as classes, avaliando a razão entre a variabilidade entre as classes (inter-classe) e a variabilidade dentro das classes (intra-classe) para cada característica. Ou seja, este favorece características que conseguem distinguir bem entre as diferentes classes. Já o ReliefF foca na diferença entre vizinhos da mesma classe e vizinhos de classes diferentes, penalizando características que tornam as classes mais semelhantes e recompensando aquelas que ajudam a distinguir as classes. Como resultado, o Fisher Score tende a selecionar características que têm uma maior separabilidade entre as classes, enquanto o ReliefF prioriza características que têm um maior poder discriminativo local entre vizinhos de classes distintas. Esse contraste explica a escolha de características diferentes entre os dois métodos, refletindo as diferentes estratégias de avaliação da importância das variáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à pergunta 5.2.4\n",
    "\n",
    "As abordagens de Feature Transformation (como o PCA) e Feature Selection (como o Fisher Score e o ReliefF) têm objetivos semelhantes, que são identificar e melhorar a representação das variáveis de entrada, mas diferem significativamente na forma como alcançam esse objetivo.\n",
    "\n",
    "O Principal Component Analysis (PCA), utilizado na Feature Transformation, transforma as variáveis originais num novo conjunto de variáveis, chamadas de componentes principais, que são combinações lineares das variáveis originais. O principal objetivo do PCA é reduzir a dimensionalidade do conjunto de dados enquanto mantém a maior parte da variabilidade (informação) presente nos dados. Uma das vantagens do PCA é a sua capacidade de combinar as características em componentes que capturam a maior parte da variação nos dados, facilitando a visualização e análise dos dados em espaços de menor dimensão. Contudo, uma desvantagem do PCA é que as novas componentes podem ser difíceis de interpretar, pois são combinações das características originais. Além disso, o PCA é sensível a escalas diferentes das variáveis, o que torna a normalização das características uma etapa importante.\n",
    "\n",
    "Por outro lado, as técnicas de Feature Selection (como o Fisher Score e o ReliefF) não transformam os dados, mas selecionam um subconjunto das variáveis originais que são consideradas mais relevantes para o modelo. O Fisher Score é um método estatístico que avalia a separabilidade entre as classes, escolhendo as características que maximizam a diferença entre elas. O ReliefF, por sua vez, baseia-se na ideia de distâncias entre pontos de dados próximos e pode capturar interações não-lineares, sendo mais eficaz para problemas em que as relações entre as variáveis são complexas. Uma vantagem dessas técnicas é que elas mantêm as características originais, o que pode facilitar a interpretação dos resultados. Além disso, elas ajudam a eliminar características irrelevantes ou redundantes, podendo melhorar a performance de modelos de aprendizagem computacional. No entanto, uma limitação dessas abordagens é que elas podem ser sensíveis a outliers e, dependendo do método, podem não capturar bem interações não-lineares ou relações complexas entre as variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#TTSplit \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "#TVT com TT split nested \n",
    "\n",
    "def nested_tvt_split(X, Y, test_size=0.3, validation_size=0.3, random_state=42):\n",
    "\n",
    "    X_train_full, X_test, Y_train_full, Y_test = train_test_split(\n",
    "        X, Y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Inner Train-Validation split\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train_full, Y_train_full, test_size=validation_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
    "\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = nested_tvt_split(X, Y)\n",
    "\n",
    "\n",
    "#K-Folds \n",
    "\n",
    "def k_fold_cross_validation(X, y, k=5, model=None):\n",
    "   \n",
    "    model = KNeighborsClassifier()  \n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)  # Initialize K-Fold\n",
    "    fold_accuracies = []  # To store accuracy of each fold\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # dividir entre teste e treino\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Treinar o modelo\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict \n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # avaliar o modelo\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Fold Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Calcular a accuracy média dos folds\n",
    "    mean_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "    print(f\"\\nMean Accuracy across {k} folds: {mean_accuracy:.2f}\")\n",
    "\n",
    "    return fold_accuracies, mean_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta à pergunta 6.1.4\n",
    "\n",
    "Cada uma das abordagens de divisão de dados tem as suas vantagens e desvantagens, dependendo do contexto e do objetivo do modelo. A abordagem Train-Test (TT) divide o conjunto de dados em dois subconjuntos: um para treino e outro para teste. A principal vantagem desta abordagem é a sua simplicidade e rapidez, sendo adequada quando temos um grande conjunto de dados e a validação do modelo não precisa de ser altamente rigorosa. No entanto, a principal desvantagem é que a avaliação do modelo pode ser muito sensível à divisão específica dos dados, não representando bem a variabilidade do conjunto total de dados, o que pode levar a uma avaliação otimista ou pessimista.\n",
    "\n",
    "A abordagem Train-Validation-Test (TVT), que utiliza uma divisão do treino em duas fases: treino e validação, permite uma avaliação mais robusta, pois o modelo é validado num conjunto de dados separado do treino, evitando o sobreajuste (overfitting). Contudo, esta abordagem é mais custosa em termos de tempo de computação, pois requer que o modelo seja treinado e validado em múltiplos subconjuntos. \n",
    "\n",
    "Por último, a K-Fold Cross-Validation é uma técnica que divide os dados em k subconjuntos (ou \"folds\") e treina e testa o modelo k vezes, cada vez com um fold diferente como conjunto de teste e os outros como conjunto de treino. A vantagem desta abordagem é que oferece uma avaliação mais robusta e menos sujeita a variações específicas de uma única divisão dos dados porque o modelo é treinado e testado em diferentes subconjuntos dos dados, permitindo que cada amostra do conjunto de dados seja utilizada tanto para treino quanto para teste. A desvantagem é o custo computacional, especialmente com grandes conjuntos de dados, pois o modelo precisa ser treinado k vezes. Em resumo, a escolha entre essas abordagens depende do tamanho do conjunto de dados, do tempo disponível para treino e do grau de robustez desejado na avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2 \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_classification_performance(y_true, y_pred):\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return {\n",
    "        \"Confusion Matrix\": conf_matrix,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementação KNN manual  \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNearestNeighbors:\n",
    "    def __init__(self, k=3, weighted=False): #o self faz com que as variáveis possam ser acedidas por toda ainstância\n",
    "       \n",
    "        self.k = k\n",
    "        self.weighted = weighted\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "      \n",
    "        self.X_train = X\n",
    "        self.Y_train = Y\n",
    "        self.Y_train = np.array(Y).flatten()\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "      \n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "       \n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            # distância de x até todos os pontos de treino\n",
    "            distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "            \n",
    "            # indices dos knn\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            \n",
    "            # labels dos knn\n",
    "            k_labels = self.Y_train[k_indices]\n",
    "            \n",
    "            # voting\n",
    "            if self.weighted:\n",
    "                # Weighted voting - inverso da distância\n",
    "                k_distances = np.array(distances)[k_indices]\n",
    "                weights = 1 / (k_distances + 1e-5)  # evitar divisão por zero\n",
    "                weighted_votes = {}\n",
    "                for label, weight in zip(k_labels, weights):\n",
    "                    weighted_votes[label] = weighted_votes.get(label, 0) + weight\n",
    "                prediction = max(weighted_votes, key=weighted_votes.get)\n",
    "            else:\n",
    "                # Majority voting se weighted voting for false\n",
    "                prediction = Counter(k_labels).most_common(1)[0][0]\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        \n",
    "        correct = np.sum(np.array(y_true) == np.array(y_pred))\n",
    "        total = len(y_true)\n",
    "        accuracy = (correct / total) * 100\n",
    "        print(accuracy)\n",
    "        return accuracy\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementação KNN com a biblioteca\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_val_pred = knn.predict(X_val)\n",
    "accuracy = accuracy_score(Y_val, Y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test split: 40%-30%-30% com KNN\n",
    "\n",
    "# normalizar\n",
    "X_loaded_normalized = normalize_zscore(X_loaded)\n",
    "\n",
    "# Split TVT\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = nested_tvt_split(X_loaded_normalized, Y_loaded)\n",
    "\n",
    "# PCA Function\n",
    "def apply_pca(X_train, X_val, X_test, variance_threshold=0.97):\n",
    "    transformed_train, components, _, _ = pca_with_explained_variance(X_train, variance_threshold)\n",
    "    transformed_val = np.dot(X_val - np.mean(X_train, axis=0), components)\n",
    "    transformed_test = np.dot(X_test - np.mean(X_train, axis=0), components)\n",
    "    return transformed_train, transformed_val, transformed_test\n",
    "\n",
    "# ReliefF Function\n",
    "def apply_relieff(X_train, X_val, X_test, relief_scores, top_n=10):\n",
    "    top_features = np.argsort(relief_scores)[-top_n:]\n",
    "    X_train_reduced = X_train[:, top_features]\n",
    "    X_val_reduced = X_val[:, top_features]\n",
    "    X_test_reduced = X_test[:, top_features]\n",
    "    return X_train_reduced, X_val_reduced, X_test_reduced\n",
    "\n",
    "# KNN Cross-Validation Function\n",
    "def cross_validate_knn(X_train, Y_train, k_values, X_val=None, Y_val=None):\n",
    "    # Results dictionary\n",
    "    results = {\"k\": [], \"accuracy\": [], \"BIC\": []}\n",
    "\n",
    "    for k in k_values:\n",
    "        # Train the KNN model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "        knn.fit(X_train, Y_train.ravel())\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        Y_val_pred = knn.predict(X_val)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "\n",
    "        # Calculate BIC\n",
    "        mse = np.mean((np.array(Y_val) - np.array(Y_val_pred)) ** 2)\n",
    "        n = len(Y_val)\n",
    "        bic = n * np.log(mse) + k * np.log(n) if mse > 0 else float('inf')  # Avoid log(0)\n",
    "\n",
    "        # Store results\n",
    "        results[\"k\"].append(k)\n",
    "        results[\"accuracy\"].append(accuracy)\n",
    "        results[\"BIC\"].append(bic)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Evaluate for All Features\n",
    "k_values = [3, 5, 10, 15, 20]\n",
    "results_all = cross_validate_knn(X_train, Y_train, k_values, X_val, Y_val)\n",
    "\n",
    "# Evaluate for PCA Features\n",
    "X_train_pca, X_val_pca, X_test_pca = apply_pca(X_train, X_val, X_test)\n",
    "results_pca = cross_validate_knn(X_train_pca, Y_train, k_values, X_val_pca, Y_val)\n",
    "\n",
    "# Evaluate for ReliefF Features\n",
    "relief_scores = reliefF(X_train, Y_train)\n",
    "X_train_relieff, X_val_relieff, X_test_relieff = apply_relieff(X_train, X_val, X_test, relief_scores)\n",
    "results_relieff = cross_validate_knn(X_train_relieff, Y_train, k_values, X_val_relieff, Y_val)\n",
    "\n",
    "# Print Results\n",
    "print(\"All Features:\")\n",
    "for k, acc, bic in zip(results_all[\"k\"], results_all[\"accuracy\"], results_all[\"BIC\"]):\n",
    "    print(f\"k={k}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n",
    "print(\"\\nPCA Features:\")\n",
    "for k, acc, bic in zip(results_pca[\"k\"], results_pca[\"accuracy\"], results_pca[\"BIC\"]):\n",
    "    print(f\"k={k}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n",
    "print(\"\\nReliefF Features:\")\n",
    "for k, acc, bic in zip(results_relieff[\"k\"], results_relieff[\"accuracy\"], results_relieff[\"BIC\"]):\n",
    "    print(f\"k={k}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split: 70%-30%; followed by 5-Fold Cross Validation on the training set com KNN\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train-Test Split (70%-30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_loaded_normalized, Y_loaded, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN Cross-Validation Function\n",
    "def cross_validate_knn(X_train, Y_train, k_values, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # resultados\n",
    "    results = {\"k\": [], \"accuracy\": [], \"BIC\": []}\n",
    "\n",
    "    for k in k_values:\n",
    "        fold_accuracies = []\n",
    "        fold_bics = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "            Y_train_fold, Y_val_fold = Y_train[train_idx], Y_train[val_idx]\n",
    "\n",
    "            # Treinar e predict com KNN \n",
    "            knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "            knn.fit(X_train_fold, Y_train_fold)\n",
    "            Y_val_pred = knn.predict(X_val_fold)\n",
    "\n",
    "            # calcular accuracy\n",
    "            accuracy = accuracy_score(Y_val_fold, Y_val_pred)\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "            # Calcular BIC\n",
    "            mse = np.mean((np.array(Y_val_fold) - np.array(Y_val_pred)) ** 2)\n",
    "            bic = len(Y_val_fold) * np.log(mse) + k * np.log(len(Y_val_fold)) if mse > 0 else float('inf')\n",
    "            fold_bics.append(bic)\n",
    "\n",
    "        # resultados médios por fold \n",
    "        mean_accuracy = np.mean(fold_accuracies)\n",
    "        mean_bic = np.mean(fold_bics)\n",
    "\n",
    "        results[\"k\"].append(k)\n",
    "        results[\"accuracy\"].append(mean_accuracy)\n",
    "        results[\"BIC\"].append(mean_bic)\n",
    "\n",
    "    return results\n",
    "\n",
    "# PCA \n",
    "def apply_pca(X_train, variance_threshold=0.99):\n",
    "    transformed_train, components, _, _ = pca_with_explained_variance(X_train, variance_threshold)\n",
    "    return transformed_train, components\n",
    "\n",
    "# ReliefF \n",
    "def apply_relieff(X_train, relief_scores, top_n=10):\n",
    "    top_features = np.argsort(relief_scores)[-top_n:]\n",
    "    return X_train[:, top_features]\n",
    "\n",
    "# All Features\n",
    "k_values = [3, 5, 10, 15, 20]\n",
    "results_all = cross_validate_knn(X_train, Y_train.ravel(), k_values)\n",
    "\n",
    "# PCA Features\n",
    "X_train_pca, _ = apply_pca(X_train)\n",
    "results_pca = cross_validate_knn(X_train_pca, Y_train.ravel(), k_values)\n",
    "\n",
    "# ReliefF Features\n",
    "relief_scores = reliefF(X_train, Y_train)\n",
    "X_train_relieff = apply_relieff(X_train, relief_scores)\n",
    "results_relieff = cross_validate_knn(X_train_relieff, Y_train.ravel(), k_values)\n",
    "\n",
    "# Resultados\n",
    "print(\"All Features:\")\n",
    "for k, acc, bic in zip(results_all[\"k\"], results_all[\"accuracy\"], results_all[\"BIC\"]):\n",
    "    print(f\"k={k}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n",
    "print(\"\\nPCA Features:\")\n",
    "for k, acc, bic in zip(results_pca[\"k\"], results_pca[\"accuracy\"], results_pca[\"BIC\"]):\n",
    "    print(f\"k={k}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n",
    "print(\"\\nReliefF Features:\")\n",
    "for k, acc, bic in zip(results_relieff[\"k\"], results_relieff[\"accuracy\"], results_relieff[\"BIC\"]):\n",
    "    print(f\"k={k}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split: 70%-30%; followed by 5-Fold Cross Validation on the training set com Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train-Test Split (70%-30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_loaded_normalized, Y_loaded, test_size=0.3, random_state=42)\n",
    "\n",
    "# KNN Cross-Validation Function\n",
    "def cross_validate_rf(X_train, Y_train, n_estimators_values, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # resultados\n",
    "    results = {\"n_estimators\": [], \"accuracy\": [], \"BIC\": []}\n",
    "\n",
    "    for n_estimators in n_estimators_values:\n",
    "        fold_accuracies = []\n",
    "        fold_bics = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "            Y_train_fold, Y_val_fold = Y_train[train_idx], Y_train[val_idx]\n",
    "\n",
    "            # Treinar e predict com Random Forest\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "            rf.fit(X_train_fold, Y_train_fold.ravel())\n",
    "            Y_val_pred = rf.predict(X_val_fold)\n",
    "\n",
    "            # Calcular accuracy\n",
    "            accuracy = accuracy_score(Y_val_fold, Y_val_pred)\n",
    "            fold_accuracies.append(accuracy)\n",
    "\n",
    "            # Calcular BIC\n",
    "            mse = np.mean((np.array(Y_val_fold) - np.array(Y_val_pred)) ** 2)\n",
    "            bic = len(Y_val_fold) * np.log(mse) + n_estimators * np.log(len(Y_val_fold)) if mse > 0 else float('inf')\n",
    "            fold_bics.append(bic)\n",
    "\n",
    "        # resultados médios por folds\n",
    "        mean_accuracy = np.mean(fold_accuracies)\n",
    "        mean_bic = np.mean(fold_bics)\n",
    "\n",
    "        results[\"n_estimators\"].append(n_estimators)\n",
    "        results[\"accuracy\"].append(mean_accuracy)\n",
    "        results[\"BIC\"].append(mean_bic)\n",
    "\n",
    "    return results\n",
    "\n",
    "# PCA \n",
    "def apply_pca(X_train, variance_threshold=0.97):\n",
    "    transformed_train, components, _, _ = pca_with_explained_variance(X_train, variance_threshold)\n",
    "    return transformed_train, components\n",
    "\n",
    "# ReliefF \n",
    "def apply_relieff(X_train, relief_scores, top_n=10):\n",
    "    top_features = np.argsort(relief_scores)[-top_n:]\n",
    "    return X_train[:, top_features]\n",
    "\n",
    "# All Features\n",
    "n_estimators_values = [10, 50, 100, 200, 500]\n",
    "results_all = cross_validate_rf(X_train, Y_train, n_estimators_values)\n",
    "\n",
    "# PCA Features\n",
    "X_train_pca, _ = apply_pca(X_train)\n",
    "results_pca = cross_validate_rf(X_train_pca, Y_train, n_estimators_values)\n",
    "\n",
    "# ReliefF Features\n",
    "relief_scores = reliefF(X_train, Y_train.ravel())\n",
    "X_train_relieff = apply_relieff(X_train, relief_scores)\n",
    "results_relieff = cross_validate_rf(X_train_relieff, Y_train, n_estimators_values)\n",
    "\n",
    "# Resultados\n",
    "print(\"All Features:\")\n",
    "for n_est, acc, bic in zip(results_all[\"n_estimators\"], results_all[\"accuracy\"], results_all[\"BIC\"]):\n",
    "    print(f\"n_estimators={n_est}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n",
    "print(\"\\nPCA Features:\")\n",
    "for n_est, acc, bic in zip(results_pca[\"n_estimators\"], results_pca[\"accuracy\"], results_pca[\"BIC\"]):\n",
    "    print(f\"n_estimators={n_est}: Accuracy={acc:.4f}, BIC={bic:.2f}\")\n",
    "\n",
    "print(\"\\nReliefF Features:\")\n",
    "for n_est, acc, bic in zip(results_relieff[\"n_estimators\"], results_relieff[\"accuracy\"], results_relieff[\"BIC\"]):\n",
    "    print(f\"n_estimators={n_est}: Accuracy={acc:.4f}, BIC={bic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionei o modelo com melhor accuraccy - Random Forest Classifier com n_estimators=500 com todas as features\n",
    "rf_classifier = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf_classifier.fit(X_train, Y_train.ravel())\n",
    "\n",
    "# Guardar o melhor modelo com pickle\n",
    "with open('random_forest_wheelchair_game.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_classifier, file)\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
